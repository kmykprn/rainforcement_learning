import torch
from typing import List, Tuple
from src.dqn.types.statictype import ExperienceDQN
from src.dqn.core.replaybuffer import ReplayBuffer
from src.dqn.models.simpleDQN import SimpleDQN
from torch.utils.tensorboard.writer import SummaryWriter


def experiences_to_tensor(
    experiences: List[ExperienceDQN], actions_list: List[str], device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    ReplayBufferã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸçµŒé¨“ã‚’tensorã«å¤‰æ›ã™ã‚‹é–¢æ•°

    Args:
        sampled_experiences: ReplayBufferã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸçµŒé¨“
        actions: å–ã‚Šã†ã‚‹è¡Œå‹•ã®ãƒªã‚¹ãƒˆ (ä¾‹: ["up", "down", "left", "right"])
        device: ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±

    Returns:
        states: çŠ¶æ…‹ã‚’æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([[1., 1.], [1., 2.], [1., 3.]], device='cuda:0')

        action_indices: è¡Œå‹•ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã€æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([0, 0, 3], device='cuda:0')

        rewards: å³æ™‚å ±é…¬ã‚’æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([10., 15., 20.], device='cuda:0')

        next_states: æ¬¡ã®çŠ¶æ…‹ã‚’æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([[1., 2.], [1., 3.], [2., 3.]], device='cuda:0')

        dones: next_statesãŒã‚´ãƒ¼ãƒ«ã‹å¦ã‹ã‚’True/Falseã§æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([0., 0., 1.], device='cuda:0')
    """

    # statesã®ä¾‹. tensor([[1., 1.], [1., 2.], [1., 3.]], device='cuda:0')
    states = torch.tensor([e["state"] for e in experiences], dtype=torch.float32).to(
        device
    )

    # è¡Œå‹•ã¯strãªã®ã§ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å€¤ã«å¤‰æ›ã—ã¦ã‹ã‚‰tensoråŒ–
    # actionsã®ä¾‹. tensor([0, 0, 3], device='cuda:0')
    action_indices = torch.tensor(
        [actions_list.index(e["action"]) for e in experiences], dtype=torch.int64
    ).to(device)

    # rewardsã®ä¾‹. tensor([10., 15., 20.], device='cuda:0')
    # (batch_size, 1) ã«ãªã‚‹
    rewards = (
        torch.tensor([e["reward"] for e in experiences], dtype=torch.float32)
        .unsqueeze(1)
        .to(device)
    )

    # next_statesã®ä¾‹. tensor([[1., 2.], [1., 3.], [2., 3.]], device='cuda:0')
    next_states = torch.tensor(
        [e["next_state"] for e in experiences], dtype=torch.float32
    ).to(device)

    # donesã®ä¾‹. tensor([0., 0., 1.], device='cuda:0')
    dones = torch.tensor([e["done"] for e in experiences], dtype=torch.float32).to(
        device
    )

    return states, action_indices, rewards, next_states, dones


def estimate_q_values(
    current_states: torch.Tensor,
    action_indices: torch.Tensor,
    model: SimpleDQN,
) -> torch.Tensor:
    """
    ç¾åœ¨ã®çŠ¶æ…‹, å–ã£ãŸè¡Œå‹•ã«ãŠã‘ã‚‹ä¾¡å€¤ï¼ˆQ[state][action]ï¼‰ã‚’æ¨è«–ã™ã‚‹é–¢æ•°

    Args:
        current_states: ç¾åœ¨ã®çŠ¶æ…‹ã‚’æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([[1., 1.], [1., 2.], [1., 3.]], device='cuda:0')

        action_indices: å®Ÿéš›ã«å–ã£ãŸè¡Œå‹•ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã€æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([0, 0, 3], device='cuda:0')

        model: Qå€¤ã‚’æ¨è«–ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«
            å‡ºåŠ›ä¾‹. tensor(
            [
                [-0.0801,  0.2743, -0.2159, -0.0567], # çŠ¶æ…‹1ã®è¡Œå‹•ç¢ºç‡
                [-0.1170,  0.5293, -0.1881, -0.0054], # çŠ¶æ…‹2ã®è¡Œå‹•ç¢ºç‡
                [-0.1662,  0.7667, -0.1642,  0.0643]  # çŠ¶æ…‹3ã®è¡Œå‹•ç¢ºç‡
            ], device='cuda:0', grad_fn=<AddmmBackward0>)

    Returns:
        torch.Tensor:  ç¾åœ¨ã®çŠ¶æ…‹, å–ã£ãŸè¡Œå‹•ã«ãŠã‘ã‚‹ä¾¡å€¤ï¼ˆQ[state][action]ï¼‰
            ä¾‹. tensor([-0.0801, -0.1170, 0.0643])
    """

    # çŠ¶æ…‹ã”ã¨ã«ã€ã™ã¹ã¦ã®è¡Œå‹•ã«ãŠã‘ã‚‹ä¾¡å€¤ï¼ˆQ[state]ï¼‰ã‚’æ¨è«–
    q_values = model(current_states)

    # å®Ÿéš›ã«å–ã£ãŸè¡Œå‹•ã«ãŠã‘ã‚‹ä¾¡å€¤ï¼ˆQ[state][action]ï¼‰ã®ã¿æŠ½å‡º
    selected_q_values_list = [q[a] for q, a in zip(q_values, action_indices)]

    # torch.stack ã§ãƒªã‚¹ãƒˆå†…ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’çµåˆ
    # ã“ã‚Œã«ã‚ˆã‚Šã€è¨ˆç®—ã‚°ãƒ©ãƒ•ãŒç¶­æŒã•ã‚ŒãŸã¾ã¾1æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ« (batch_size,) ãŒå¾—ã‚‰ã‚Œã‚‹
    estimated_q = torch.stack(selected_q_values_list)

    # å‡ºåŠ›ã¯ (batch_size,) ã®å½¢çŠ¶ãªã®ã§ã€(batch_size, 1)ã«å¤‰æ›
    estimated_q = estimated_q.unsqueeze(1)

    # 1æ¬¡å…ƒã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    return estimated_q


def estimate_next_q_max_values(
    next_states: torch.Tensor,
    dones: torch.Tensor,
    target_model: SimpleDQN,
    device: torch.device,
) -> torch.Tensor:
    """
    æ¬¡ã®çŠ¶æ…‹ã«ãŠã‘ã‚‹ä¾¡å€¤æœ€å¤§ã®è¡Œå‹•ã®Qå€¤ï¼ˆQ[state][max(action)]ï¼‰ã‚’æ¨è«–ã™ã‚‹é–¢æ•°

    Args:
        next_states: æ¬¡ã®çŠ¶æ…‹ã‚’æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([[1., 1.], [1., 2.], [1., 3.]], device='cuda:0')

        dones: next_statesãŒã‚´ãƒ¼ãƒ«ã‹å¦ã‹ã‚’True/Falseã§æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([0., 0., 1.], device='cuda:0')

        target_model: Qå€¤ã‚’æ¨è«–ã™ã‚‹ãŸã‚ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«
            å‡ºåŠ›ä¾‹. tensor(
            [
                [-0.0801,  0.2743, -0.2159, -0.0567], # çŠ¶æ…‹1ã®è¡Œå‹•ç¢ºç‡
                [-0.1170,  0.5293, -0.1881, -0.0054], # çŠ¶æ…‹2ã®è¡Œå‹•ç¢ºç‡
                [-0.1662,  0.7667, -0.1642,  0.0643]  # çŠ¶æ…‹3ã®è¡Œå‹•ç¢ºç‡
            ], device='cuda:0', grad_fn=<AddmmBackward0>)

        device: gpu or cpu

    Returns:
        torch.Tensor:  æ¬¡ã®çŠ¶æ…‹ã«ãŠã‘ã‚‹ä¾¡å€¤æœ€å¤§ã®è¡Œå‹•ã®Qå€¤ï¼ˆQ[state][max(action)]ï¼‰ã€‚
        å‹¾é…ã¯è¨ˆç®—ã•ã‚Œãªã„ã€‚
            ä¾‹. tensor([0.2743, 0.5293, 0.7667])
    """

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨ˆç®—ã§ã¯å‹¾é…ã¯ä¸è¦
    with torch.no_grad():

        # çŠ¶æ…‹ã”ã¨ã«ã€ã™ã¹ã¦ã®è¡Œå‹•ã«ãŠã‘ã‚‹ä¾¡å€¤ï¼ˆQ[state]ï¼‰ã‚’æ¨è«–
        next_q_values = target_model(next_states)

        # ä¾¡å€¤æœ€å¤§ã®è¡Œå‹•ï¼ˆQ[next_state][max(action)]ï¼‰ã®ã¿æŠ½å‡º
        next_q_values_max_list = [max(q) for q in next_q_values]

        # next_statesãŒã‚´ãƒ¼ãƒ«ã®å ´åˆã€ä»¥é™ã¯ä¾¡å€¤ãŒå¾—ã‚‰ã‚Œãªã„ãŸã‚è¡Œå‹•ç¢ºç‡(Qå€¤)ã‚’0ã«ã™ã‚‹
        for i, done in enumerate(dones):
            if done:
                next_q_values_max_list[i] = 0.0

        # æˆ»ã‚Šå€¤ã‚’torch.tensor(batch, 1)ã§è¿”ã™
        next_q_values_max = torch.tensor(
            next_q_values_max_list, dtype=torch.float32, device=device
        ).unsqueeze(1)

    # 1æ¬¡å…ƒã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    return next_q_values_max


def estimate_next_q_double_dqn(
    next_states: torch.Tensor,
    dones: torch.Tensor,
    model: SimpleDQN,  # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    target_model: SimpleDQN,  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    device: torch.device,
) -> torch.Tensor:
    """
    Double DQNç”¨ã®æ¬¡ã®çŠ¶æ…‹ã«ãŠã‘ã‚‹ä¾¡å€¤æœ€å¤§ã®è¡Œå‹•ã®Qå€¤ã‚’æ¨è«–ã™ã‚‹é–¢æ•°

    ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡Œå‹•ã‚’é¸æŠã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ãã®è¡Œå‹•ã®ä¾¡å€¤ã‚’è©•ä¾¡ã™ã‚‹

    Args:
        next_states: æ¬¡ã®çŠ¶æ…‹ã‚’æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([[1., 1.], [1., 2.], [1., 3.]], device='cuda:0')

        dones: next_statesãŒã‚´ãƒ¼ãƒ«ã‹å¦ã‹ã‚’True/Falseã§æ ¼ç´ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
            ä¾‹: tensor([0., 0., 1.], device='cuda:0')

        model: è¡Œå‹•é¸æŠç”¨ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

        target_model: è¡Œå‹•è©•ä¾¡ç”¨ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

        device: gpu or cpu

    Returns:
        torch.Tensor: Double DQNã§è¨ˆç®—ã—ãŸæ¬¡ã®çŠ¶æ…‹ã«ãŠã‘ã‚‹ä¾¡å€¤
    """

    # å‹¾é…è¨ˆç®—ã¯ä¸è¦
    with torch.no_grad():
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§æ¬¡ã®çŠ¶æ…‹ã§ã®è¡Œå‹•ã®ä¾¡å€¤ã‚’å–å¾—(shape: batch, 4)
        next_q_values_online = model(next_states)

        # æ¬¡ã®çŠ¶æ…‹ã«ãŠã‘ã‚‹è¡Œå‹•é¸æŠã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å®Ÿæ–½ã€‚
        # å„çŠ¶æ…‹ã§ã®æœ€å¤§Qå€¤ã‚’æŒã¤è¡Œå‹•ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—(shape: batch, 1)
        # dim=1ã§ã€è¡Œå‹•ã®æ¬¡å…ƒã‚’è¦‹ã¦argmaxã‚’å–å¾—
        best_actions = next_q_values_online.argmax(dim=1, keepdim=True)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚Šé¸æŠã•ã‚ŒãŸçŠ¶æ…‹, è¡Œå‹•ã®ä¾¡å€¤ã‚’å–å¾—(shape: batch, 4)
        next_q_values_target = target_model(next_states)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã€æ¬¡ã®çŠ¶æ…‹ã§ãƒ™ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ä¾¡å€¤ã‚’å–å¾—
        # .gather()ã¯ã€dimã§æŒ‡å®šã—ãŸè»¸ï¼ˆä»Šå›ã¯è¡Œå‹•ã®æ¬¡å…ƒï¼‰ã«å¯¾ã—ã€indexã§æŒ‡å®šã—ãŸè¡Œå‹•ã‚’å–å¾—
        # ãªãœã“ã‚Œã§å®‰å®šæ€§ãŒç¢ºä¿ã§ãã‚‹ã‹ãƒ¢ãƒ¤ãƒ¢ãƒ¤ã¯æ®‹ã‚‹ãŒã€ä¸€æ—¦æ‰‹æ³•è‡ªä½“ã¯ç†è§£ã—ãŸã®ã§å…ˆã«é€²ã‚€ã€‚
        next_q_values = next_q_values_target.gather(
            dim=1, index=best_actions
        )  # shape: (batch, 1)

        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§é¸æŠã•ã‚ŒãŸè¡Œå‹•ã®ä¾¡å€¤ã‚’å–å¾—
        next_q_values_list = []
        for i, action_idx in enumerate(best_actions):
            next_q_values_list.append(next_q_values_target[i][action_idx].item())

        # next_statesãŒã‚´ãƒ¼ãƒ«ã®å ´åˆã€ä»¥é™ã¯ä¾¡å€¤ãŒå¾—ã‚‰ã‚Œãªã„ãŸã‚è¡Œå‹•ç¢ºç‡(Qå€¤)ã‚’0ã«ã™ã‚‹
        for i, done in enumerate(dones):
            if done:
                next_q_values_list[i] = 0.0

        # æˆ»ã‚Šå€¤ã‚’torch.tensor(batch, 1)ã§è¿”ã™
        next_q_values = torch.tensor(
            next_q_values_list, dtype=torch.float32, device=device
        ).unsqueeze(1)

    return next_q_values


def train_rl(
    device: torch.device,
    model: SimpleDQN,
    target_model: SimpleDQN,
    optimizer: torch.optim.Optimizer,
    replay_buffer: ReplayBuffer,
    actions_list: List[str],  # actions ã‚’å¼•æ•°ã§å—ã‘å–ã‚‹
    batch_size: int,
    writer: SummaryWriter,
    gamma: float = 0.99,
    global_step: int = 0,
):
    """
    æ·±å±¤å¼·åŒ–å­¦ç¿’ã®å­¦ç¿’ç”¨é–¢æ•°
    """

    # ååˆ†ãªãƒ‡ãƒ¼ã‚¿é‡ãŒãŸã¾ã‚‹ã¾ã§å­¦ç¿’ã¯ã—ãªã„
    if len(replay_buffer) < batch_size:
        return

    # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
    model.train()

    # ReplayBufferã‹ã‚‰çµŒé¨“ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    experiences: List[ExperienceDQN] = replay_buffer.sample(batch_size)

    # çµŒé¨“ãƒ†ãƒ³ã‚½ãƒ«ã«æ ¼ç´
    states, action_indices, rewards, next_states, dones = experiences_to_tensor(
        experiences, actions_list, device
    )

    # ç¾åœ¨ã®çŠ¶æ…‹, å–ã£ãŸè¡Œå‹•ã«ãŠã‘ã‚‹ä¾¡å€¤ï¼ˆQ[state][action]ï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–
    estimated_q = estimate_q_values(states, action_indices, model)

    # æ¬¡ã®çŠ¶æ…‹ã«ãŠã‘ã‚‹Qå€¤ã®ä¸­ã‹ã‚‰æœ€å¤§ã®å€¤ã‚’å–ã‚Šå‡ºã—
    # target_modelã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€TDèª¤å·®ã‚’å®‰å®šåŒ–ã™ã‚‹
    next_q_values_max = estimate_next_q_max_values(
        next_states, dones, target_model, device
    )

    # å®Ÿéš›ã«å¾—ã‚‰ã‚ŒãŸä¾¡å€¤ï¼ˆQå€¤ï¼‰ã‚’è¨ˆç®—
    gain_q = rewards + gamma * next_q_values_max

    # å‰å›ã®å‹¾é…ã‚’ãƒªã‚»ãƒƒãƒˆ
    optimizer.zero_grad()

    # Huberæå¤±
    loss = torch.nn.functional.smooth_l1_loss(estimated_q, gain_q)

    # å‹¾é…ã‚’è¨ˆç®—
    loss.backward()

    # é‡ã¿ã‚’æ›´æ–°
    optimizer.step()

    if writer:  # ğŸ‘ˆ TensorBoardã‚’ä½¿ã†ãŸã‚ã«è¿½åŠ 
        writer.add_scalar("Loss/train", loss.item(), global_step)

    if global_step % 100 == 0:
        loss = loss.item()
        print(f"loss: {loss:.7f} count: {global_step: 5d}")


def train_double_dqn(
    device: torch.device,
    model: SimpleDQN,
    target_model: SimpleDQN,
    optimizer: torch.optim.Optimizer,
    replay_buffer: ReplayBuffer,
    actions_list: List[str],
    batch_size: int,
    writer: SummaryWriter,
    gamma: float = 0.99,
    global_step: int = 0,
):
    """
    Double DQNã®å­¦ç¿’ç”¨é–¢æ•°

    é€šå¸¸ã®DQNã¨ã®é•ã„ã¯ã€æ¬¡ã®çŠ¶æ…‹ã§ã®è¡Œå‹•é¸æŠã¨è©•ä¾¡ã‚’åˆ¥ã€…ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡Œã†ã“ã¨
    """

    # ååˆ†ãªãƒ‡ãƒ¼ã‚¿é‡ãŒãŸã¾ã‚‹ã¾ã§å­¦ç¿’ã¯ã—ãªã„
    if len(replay_buffer) < batch_size:
        return

    # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
    model.train()

    # ReplayBufferã‹ã‚‰çµŒé¨“ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    experiences: List[ExperienceDQN] = replay_buffer.sample(batch_size)

    # çµŒé¨“ãƒ†ãƒ³ã‚½ãƒ«ã«æ ¼ç´
    states, action_indices, rewards, next_states, dones = experiences_to_tensor(
        experiences, actions_list, device
    )

    # ç¾åœ¨ã®çŠ¶æ…‹, å–ã£ãŸè¡Œå‹•ã«ãŠã‘ã‚‹ä¾¡å€¤ï¼ˆQ[state][action]ï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–
    estimated_q = estimate_q_values(states, action_indices, model)

    # Double DQN: æ¬¡ã®çŠ¶æ…‹ã§ã®è¡Œå‹•é¸æŠã¨è©•ä¾¡ã‚’åˆ¥ã€…ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡Œã†
    # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(model)ã§è¡Œå‹•ã‚’é¸æŠã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(target_model)ã§ãã®è¡Œå‹•ã®ä¾¡å€¤ã‚’è©•ä¾¡
    next_q_values = estimate_next_q_double_dqn(
        next_states, dones, model, target_model, device
    )

    # å®Ÿéš›ã«å¾—ã‚‰ã‚ŒãŸä¾¡å€¤ï¼ˆQå€¤ï¼‰ã‚’è¨ˆç®—
    gain_q = rewards + gamma * next_q_values

    # å‰å›ã®å‹¾é…ã‚’ãƒªã‚»ãƒƒãƒˆ
    optimizer.zero_grad()

    # Huberæå¤±
    loss = torch.nn.functional.smooth_l1_loss(estimated_q, gain_q)

    # å‹¾é…ã‚’è¨ˆç®—
    loss.backward()

    # é‡ã¿ã‚’æ›´æ–°
    optimizer.step()

    if writer:
        writer.add_scalar("Loss/train", loss.item(), global_step)

    if global_step % 100 == 0:
        loss = loss.item()
        print(f"loss: {loss:.7f} count: {global_step: 5d}")
