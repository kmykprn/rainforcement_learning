## これは何？
強化学習（深層学習連携前）のpython実装。
極力外部ライブラリは使わず実装した。

## 動作環境
Python 3.10.0

## 使い方
### 事前作業
```
cd reinforcement_learning
poetry install
```

### コードの実施：
各ディレクトリに train.py が格納されており、以下のように実行すると学習が始まる。
```
cd reinforcement_learning
poetry run python 03_Qlearning/train.py
```

### 各ディレクトリの説明：
- 01_MDP: マルコフ決定過程の実装
- 02_MonteCarlo: モンテカルロ法の実装
- 03_Qlearning: Qlearningの実装
- 04_SARSA: SARSAの実装
- 05_ActorCritic: ActorCriticの実装
- config: 学習回数などのハイパーパラメータを格納
- core: 強化学習のコア処理（ポリシー、環境など）を格納
- utils: 上記以外の共通処理を格納


### Double DQNの要点
```
- DQNでは：
    - ターゲットネットワークが「次の状態における全行動のQ値」を推定し、その中で最大のQ値（max Q）を選ぶ。
    - 行動の選択と、その行動に対するQ値の評価の両方を同じネットワーク（target）で行っていたため、
      Q値のノイズや推定ミスが重なると「本来望ましくない行動」が選ばれ、過大な価値を与えてしまう。
    - さらに、ターゲットネットワークは更新頻度が低いため、その偏りが長く維持されてしまい、TD誤差（＝Loss）が発散しやすくなる。
- Double DQNでは：
    - ターゲットネットワークの行動選択部分を分離し、オンラインネットワーク側で行動選択を行なう。
    - ターゲットネットワークを使っての行動選択を行なわないため、TD誤差の発散が抑えられるようになる
    - 行動選択はオンラインモデル（頻繁に更新される）で行うため、より「現状の最適行動」に近いものを選びやすい
    - 選ばれた行動に対するQ値の評価は、安定したターゲットネットワーク（target_model）で行うため、価値推定がブレにくい。
    - この分離により、ノイズによる誤った行動選択に引っ張られることが少なくなり、TD誤差の暴走を防ぎ、学習の安定性が向上する。
```
